#!/usr/bin/env python3
"""
Complete Pipeline Execution & Validation
========================================

ì „ì²´ ML íŒŒì´í”„ë¼ì¸ ì¬êµ¬ì¶• ë° ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

ì‹¤í–‰ ìˆœì„œ:
1. ë°ì´í„° ëˆ„ìˆ˜ ì—†ëŠ” ë¼ë²¨ë§ ì‹œìŠ¤í…œ ì‹¤í–‰
2. ì™¸ë¶€ ì§€í‘œ ê¸°ë°˜ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹¤í–‰
3. ê³¼ì í•© ë°©ì§€ ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨
4. í†µí•© ë§ˆìŠ¤í„° íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
5. ì„±ëŠ¥ ê²€ì¦ ë° ìµœì¢… ë³´ê³ ì„œ

ì‚¬ìš©ë²•:
python run_complete_pipeline.py [--skip-training] [--quick-test]

Author: Seoul Market Risk ML System - Complete Integration
Date: 2025-09-17
"""

import subprocess
import sys
import time
import json
from pathlib import Path
from typing import Dict, List, Tuple
import argparse

class CompleteMLPipeline:
    """ì „ì²´ ML íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë° ê²€ì¦"""

    def __init__(self, skip_training: bool = False, quick_test: bool = False):
        self.skip_training = skip_training
        self.quick_test = quick_test
        self.execution_log = []
        self.start_time = time.time()

    def log_step(self, step: str, status: str, details: str = "", duration: float = 0):
        """ì‹¤í–‰ ë¡œê·¸ ê¸°ë¡"""
        self.execution_log.append({
            'step': step,
            'status': status,
            'details': details,
            'duration': duration,
            'timestamp': time.time() - self.start_time
        })

        # ì½˜ì†” ì¶œë ¥
        status_emoji = "âœ…" if status == "SUCCESS" else "âŒ" if status == "FAILED" else "ğŸ”„"
        print(f"{status_emoji} {step}")
        if details:
            print(f"   {details}")
        if duration > 0:
            print(f"   Duration: {duration:.1f}s")

    def run_python_script(self, script_name: str, description: str) -> Tuple[bool, str]:
        """Python ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰"""
        print(f"\nğŸ”„ Executing: {description}")
        print(f"   Script: {script_name}")

        start_time = time.time()

        try:
            result = subprocess.run(
                [sys.executable, script_name],
                capture_output=True,
                text=True,
                timeout=1800  # 30ë¶„ íƒ€ì„ì•„ì›ƒ
            )

            duration = time.time() - start_time

            if result.returncode == 0:
                self.log_step(description, "SUCCESS",
                            f"Exit code: {result.returncode}", duration)
                return True, result.stdout
            else:
                error_msg = result.stderr if result.stderr else result.stdout
                self.log_step(description, "FAILED",
                            f"Exit code: {result.returncode}, Error: {error_msg[:200]}", duration)
                return False, error_msg

        except subprocess.TimeoutExpired:
            self.log_step(description, "FAILED", "Timeout (30 minutes)", duration)
            return False, "Script execution timed out"
        except Exception as e:
            duration = time.time() - start_time
            self.log_step(description, "FAILED", str(e), duration)
            return False, str(e)

    def check_file_exists(self, file_path: str, description: str) -> bool:
        """íŒŒì¼ ì¡´ì¬ í™•ì¸"""
        exists = Path(file_path).exists()
        status = "SUCCESS" if exists else "FAILED"
        self.log_step(f"Check: {description}", status, f"Path: {file_path}")
        return exists

    def step1_data_labeling(self) -> bool:
        """1ë‹¨ê³„: ë°ì´í„° ëˆ„ìˆ˜ ì—†ëŠ” ë¼ë²¨ë§"""
        print("\n" + "="*60)
        print("1ï¸âƒ£ STEP 1: Data Labeling (NO LEAKAGE)")
        print("="*60)

        # 1-1. ë¼ë²¨ë§ ì‹œìŠ¤í…œ ì‹¤í–‰
        success, output = self.run_python_script(
            "fixed_data_labeling_system.py",
            "Step 1-1: Execute Fixed Data Labeling System"
        )

        if not success:
            return False

        # 1-2. ê²°ê³¼ íŒŒì¼ í™•ì¸
        return self.check_file_exists(
            "ml_analysis_results/seoul_commercial_fixed_dataset.csv",
            "Step 1-2: Fixed labeled dataset created"
        )

    def step2_feature_engineering(self) -> bool:
        """2ë‹¨ê³„: ì™¸ë¶€ ì§€í‘œ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§"""
        print("\n" + "="*60)
        print("2ï¸âƒ£ STEP 2: Feature Engineering (EXTERNAL INDICATORS)")
        print("="*60)

        # 2-1. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹¤í–‰
        success, output = self.run_python_script(
            "fixed_feature_engineering.py",
            "Step 2-1: Execute Fixed Feature Engineering"
        )

        if not success:
            return False

        # 2-2. ê²°ê³¼ íŒŒì¼ë“¤ í™•ì¸
        files_to_check = [
            "ml_preprocessed_data_fixed/train_data.csv",
            "ml_preprocessed_data_fixed/validation_data.csv",
            "ml_preprocessed_data_fixed/test_data.csv",
            "ml_preprocessed_data_fixed/class_weights.joblib"
        ]

        all_exist = True
        for file_path in files_to_check:
            if not self.check_file_exists(file_path, f"Step 2-2: {Path(file_path).name}"):
                all_exist = False

        return all_exist

    def step3_model_training(self) -> bool:
        """3ë‹¨ê³„: ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨"""
        print("\n" + "="*60)
        print("3ï¸âƒ£ STEP 3: Ensemble Model Training (OVERFITTING PREVENTION)")
        print("="*60)

        if self.skip_training:
            print("â­ï¸ Skipping model training (--skip-training flag)")
            # ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ë“¤ í™•ì¸
            model_files = list(Path("trained_models_fixed").glob("*_model.joblib"))
            if len(model_files) >= 3:
                self.log_step("Step 3: Model Training", "SKIPPED",
                            f"Found {len(model_files)} existing models")
                return True
            else:
                self.log_step("Step 3: Model Training", "FAILED",
                            "No existing models found, training required")
                return False

        # 3-1. ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰
        success, output = self.run_python_script(
            "fixed_ensemble_training.py",
            "Step 3-1: Execute Fixed Ensemble Training"
        )

        if not success:
            return False

        # 3-2. ëª¨ë¸ íŒŒì¼ë“¤ í™•ì¸
        model_files = [
            "trained_models_fixed/random_forest_model.joblib",
            "trained_models_fixed/xgboost_model.joblib",
            "trained_models_fixed/model_evaluation_results.json"
        ]

        all_exist = True
        for file_path in model_files:
            if Path(file_path).exists():
                self.log_step(f"Step 3-2: {Path(file_path).name}", "SUCCESS", f"Found: {file_path}")
            else:
                self.log_step(f"Step 3-2: {Path(file_path).name}", "WARNING", f"Missing: {file_path}")

        # ìµœì†Œ 1ê°œ ëª¨ë¸ë§Œ ìˆìœ¼ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
        existing_models = list(Path("trained_models_fixed").glob("*_model.joblib"))
        if len(existing_models) >= 1:
            self.log_step("Step 3-2: Model Training", "SUCCESS",
                        f"Created {len(existing_models)} models")
            return True
        else:
            self.log_step("Step 3-2: Model Training", "FAILED", "No models created")
            return False

    def step4_integration_test(self) -> bool:
        """4ë‹¨ê³„: í†µí•© íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
        print("\n" + "="*60)
        print("4ï¸âƒ£ STEP 4: Master Pipeline Integration Test")
        print("="*60)

        # 4-1. í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        success, output = self.run_python_script(
            "master_integrated_pipeline.py",
            "Step 4-1: Execute Master Integrated Pipeline"
        )

        if not success:
            return False

        # 4-2. ì¶œë ¥ ë¶„ì„ (ê°„ë‹¨í•œ ê²€ì¦)
        if "âœ… Master Pipeline ready!" in output:
            self.log_step("Step 4-2: Pipeline Initialization", "SUCCESS",
                        "Master pipeline initialized successfully")
        else:
            self.log_step("Step 4-2: Pipeline Initialization", "WARNING",
                        "Pipeline may have initialization issues")

        if "ğŸ¯ Hybrid Risk Prediction" in output:
            self.log_step("Step 4-3: Prediction Test", "SUCCESS",
                        "Prediction functionality working")
            return True
        else:
            self.log_step("Step 4-3: Prediction Test", "WARNING",
                        "Prediction test may have issues")
            return True  # ë¶€ë¶„ ì„±ê³µìœ¼ë¡œ ê°„ì£¼

    def step5_performance_validation(self) -> bool:
        """5ë‹¨ê³„: ì„±ëŠ¥ ê²€ì¦"""
        print("\n" + "="*60)
        print("5ï¸âƒ£ STEP 5: Performance Validation")
        print("="*60)

        # 5-1. ëª¨ë¸ ì„±ëŠ¥ ê²°ê³¼ ë¡œë“œ
        try:
            results_file = Path("trained_models_fixed/model_evaluation_results.json")
            if results_file.exists():
                with open(results_file, 'r', encoding='utf-8') as f:
                    results = json.load(f)

                # ì„±ëŠ¥ ì§€í‘œ ë¶„ì„
                best_accuracy = 0
                best_f1 = 0
                model_count = 0

                for model_name, eval_results in results.get('evaluation_results', {}).items():
                    accuracy = eval_results.get('test_accuracy', 0)
                    f1_score = eval_results.get('test_f1_weighted', 0)

                    best_accuracy = max(best_accuracy, accuracy)
                    best_f1 = max(best_f1, f1_score)
                    model_count += 1

                    print(f"   ğŸ“Š {model_name}: Accuracy={accuracy:.3f}, F1={f1_score:.3f}")

                # ì„±ëŠ¥ í‰ê°€
                if best_accuracy >= 0.8 and best_f1 >= 0.75:
                    self.log_step("Step 5-1: Performance Analysis", "SUCCESS",
                                f"Best: Accuracy={best_accuracy:.3f}, F1={best_f1:.3f}")
                    performance_pass = True
                elif best_accuracy >= 0.7 and best_f1 >= 0.65:
                    self.log_step("Step 5-1: Performance Analysis", "WARNING",
                                f"Moderate: Accuracy={best_accuracy:.3f}, F1={best_f1:.3f}")
                    performance_pass = True
                else:
                    self.log_step("Step 5-1: Performance Analysis", "FAILED",
                                f"Low: Accuracy={best_accuracy:.3f}, F1={best_f1:.3f}")
                    performance_pass = False

            else:
                self.log_step("Step 5-1: Performance Analysis", "WARNING",
                            "No evaluation results found")
                performance_pass = True  # ë¶€ë¶„ ì„±ê³µ

        except Exception as e:
            self.log_step("Step 5-1: Performance Analysis", "FAILED", str(e))
            performance_pass = False

        # 5-2. ë°ì´í„° ëˆ„ìˆ˜ ê²€ì¦
        try:
            # í”¼ì²˜ íŒŒì¼ì—ì„œ ë§¤ì¶œ ê´€ë ¨ ì»¬ëŸ¼ í™•ì¸
            train_data = Path("ml_preprocessed_data_fixed/train_data.csv")
            if train_data.exists():
                import pandas as pd
                df = pd.read_csv(train_data, nrows=1)  # í—¤ë”ë§Œ ì½ê¸°
                revenue_columns = [col for col in df.columns if 'ë§¤ì¶œ' in col or 'revenue' in col.lower()]

                if len(revenue_columns) == 0:
                    self.log_step("Step 5-2: Data Leakage Check", "SUCCESS",
                                "No revenue columns found in features")
                    leakage_pass = True
                else:
                    self.log_step("Step 5-2: Data Leakage Check", "WARNING",
                                f"Found potential revenue columns: {revenue_columns}")
                    leakage_pass = True  # ê²½ê³ ì§€ë§Œ í†µê³¼
            else:
                self.log_step("Step 5-2: Data Leakage Check", "WARNING",
                            "Cannot check - training data not found")
                leakage_pass = True

        except Exception as e:
            self.log_step("Step 5-2: Data Leakage Check", "FAILED", str(e))
            leakage_pass = False

        return performance_pass and leakage_pass

    def generate_final_report(self) -> None:
        """ìµœì¢… ë³´ê³ ì„œ ìƒì„±"""
        print("\n" + "="*70)
        print("ğŸ“‹ FINAL PIPELINE EXECUTION REPORT")
        print("="*70)

        total_duration = time.time() - self.start_time

        # ë‹¨ê³„ë³„ ìš”ì•½
        step_status = {}
        for log_entry in self.execution_log:
            step = log_entry['step']
            status = log_entry['status']

            if 'STEP' in step and ':' in step:
                step_num = step.split(':')[0].strip()
                if step_num not in step_status:
                    step_status[step_num] = {'SUCCESS': 0, 'WARNING': 0, 'FAILED': 0, 'SKIPPED': 0}
                step_status[step_num][status] = step_status[step_num].get(status, 0) + 1

        # ë‹¨ê³„ë³„ ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š Step-by-Step Results:")
        overall_success = True

        for step, counts in step_status.items():
            success_count = counts.get('SUCCESS', 0)
            warning_count = counts.get('WARNING', 0)
            failed_count = counts.get('FAILED', 0)
            skipped_count = counts.get('SKIPPED', 0)

            if failed_count > 0:
                status_emoji = "âŒ"
                overall_success = False
            elif warning_count > 0:
                status_emoji = "âš ï¸"
            elif skipped_count > 0:
                status_emoji = "â­ï¸"
            else:
                status_emoji = "âœ…"

            print(f"   {status_emoji} {step}: âœ…{success_count} âš ï¸{warning_count} âŒ{failed_count} â­ï¸{skipped_count}")

        # ì „ì²´ ê²°ê³¼
        print(f"\nğŸ¯ Overall Result:")
        if overall_success:
            print(f"   âœ… PIPELINE SUCCESS")
            print(f"   ğŸ‰ All critical components working")
            print(f"   â±ï¸ Total execution time: {total_duration:.1f}s")
        else:
            print(f"   âŒ PIPELINE FAILED")
            print(f"   ğŸ”§ Some components need attention")
            print(f"   â±ï¸ Execution time: {total_duration:.1f}s")

        # ì„¸ë¶€ ë¡œê·¸ ì €ì¥
        log_file = Path("pipeline_execution_log.json")
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump({
                'execution_log': self.execution_log,
                'total_duration': total_duration,
                'overall_success': overall_success,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ“„ Detailed log saved: {log_file}")

        # ë‹¤ìŒ ë‹¨ê³„ ì¶”ì²œ
        print(f"\nğŸš€ Next Steps:")
        if overall_success:
            print(f"   1. Review model performance in trained_models_fixed/")
            print(f"   2. Test master_integrated_pipeline.py with your data")
            print(f"   3. Deploy to production environment")
        else:
            print(f"   1. Check detailed logs for error information")
            print(f"   2. Fix failed components and re-run")
            print(f"   3. Consider running with --skip-training for faster debugging")

        print("="*70)

    def run_complete_pipeline(self) -> bool:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("ğŸš€ Starting Complete ML Pipeline Execution")
        print(f"âš™ï¸ Configuration: skip_training={self.skip_training}, quick_test={self.quick_test}")
        print("="*70)

        # ë‹¨ê³„ë³„ ì‹¤í–‰
        steps = [
            ("Step 1: Data Labeling", self.step1_data_labeling),
            ("Step 2: Feature Engineering", self.step2_feature_engineering),
            ("Step 3: Model Training", self.step3_model_training),
            ("Step 4: Integration Test", self.step4_integration_test),
            ("Step 5: Performance Validation", self.step5_performance_validation)
        ]

        failed_steps = []

        for step_name, step_function in steps:
            try:
                success = step_function()
                if not success:
                    failed_steps.append(step_name)
                    if not self.quick_test:  # quick_test ëª¨ë“œê°€ ì•„ë‹ˆë©´ ì‹¤íŒ¨ì‹œ ì¤‘ë‹¨
                        print(f"\nâŒ {step_name} failed. Stopping execution.")
                        break
            except Exception as e:
                self.log_step(step_name, "FAILED", f"Exception: {str(e)}")
                failed_steps.append(step_name)
                if not self.quick_test:
                    print(f"\nâŒ {step_name} failed with exception. Stopping execution.")
                    break

        # ìµœì¢… ë³´ê³ ì„œ ìƒì„±
        self.generate_final_report()

        return len(failed_steps) == 0

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Complete ML Pipeline Execution")
    parser.add_argument("--skip-training", action="store_true",
                       help="Skip model training step (use existing models)")
    parser.add_argument("--quick-test", action="store_true",
                       help="Continue execution even if steps fail")

    args = parser.parse_args()

    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = CompleteMLPipeline(
        skip_training=args.skip_training,
        quick_test=args.quick_test
    )

    success = pipeline.run_complete_pipeline()

    # ì¢…ë£Œ ì½”ë“œ ì„¤ì •
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()