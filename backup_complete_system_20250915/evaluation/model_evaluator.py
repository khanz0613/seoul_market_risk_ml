#!/usr/bin/env python3
"""
ì„œìš¸ ì‹œì¥ ìœ„í—˜ë„ ML ì‹œìŠ¤í…œ - ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
Seoul Market Risk ML System - Model Performance Evaluation

í›ˆë ¨ëœ ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.
"""

import sys
import os
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, 'src'))

import pandas as pd
import numpy as np
import json
from datetime import datetime
from pathlib import Path
import time
import logging
import warnings
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt
import seaborn as sns

# ML imports
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error
import joblib

# System imports
from src.utils.config_loader import load_config, get_data_paths

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SeoulModelEvaluator:
    """ì„œìš¸ ì‹œì¥ ìœ„í—˜ë„ ML ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.config = load_config()
        self.data_paths = get_data_paths(self.config)
        self.models_dir = Path("models")
        self.evaluation_results = {
            'timestamp': datetime.now(),
            'model_performance': {},
            'comparative_analysis': {},
            'recommendations': []
        }
        
    def load_trained_models(self) -> Dict:
        """í›ˆë ¨ëœ ëª¨ë¸ë“¤ ë¡œë“œ"""
        logger.info("ğŸ“š í›ˆë ¨ëœ ëª¨ë¸ë“¤ ë¡œë”© ì¤‘...")
        
        models = {}
        
        # ê¸€ë¡œë²Œ ëª¨ë¸
        global_path = self.models_dir / 'global_model.joblib'
        if global_path.exists():
            models['global'] = joblib.load(global_path)
            logger.info("   âœ… ê¸€ë¡œë²Œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # ì§€ì—­ ëª¨ë¸ë“¤
        regional_models = {}
        for model_file in self.models_dir.glob('regional_model_*.joblib'):
            region_id = model_file.stem.split('_')[-1]
            regional_models[f'region_{region_id}'] = joblib.load(model_file)
        
        if regional_models:
            models['regional'] = regional_models
            logger.info(f"   âœ… ì§€ì—­ ëª¨ë¸ {len(regional_models)}ê°œ ë¡œë“œ ì™„ë£Œ")
        
        # ë¡œì»¬ ëª¨ë¸ë“¤
        local_models = {}
        for model_file in self.models_dir.glob('local_model_*.joblib'):
            parts = model_file.stem.split('_')
            region_id = parts[2]
            business_id = parts[3]
            local_models[f'local_{region_id}_{business_id}'] = joblib.load(model_file)
        
        if local_models:
            models['local'] = local_models
            logger.info(f"   âœ… ë¡œì»¬ ëª¨ë¸ {len(local_models)}ê°œ ë¡œë“œ ì™„ë£Œ")
        
        total_models = 1 if 'global' in models else 0
        total_models += len(regional_models)
        total_models += len(local_models)
        
        logger.info(f"ì´ {total_models}ê°œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        return models
    
    def prepare_test_data(self) -> Tuple[pd.DataFrame, pd.Series]:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„"""
        logger.info("ğŸ§ª í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        
        # í›ˆë ¨ì— ì‚¬ìš©ëœ ë™ì¼í•œ ì „ì²˜ë¦¬ ì ìš©
        combined_file = self.data_paths['processed'] / 'seoul_sales_combined.csv'
        df = pd.read_csv(combined_file)
        
        # íŠ¹ì„± ì„ íƒ (í›ˆë ¨ê³¼ ë™ì¼í•˜ê²Œ)
        feature_columns = [
            'district_code', 'business_type_code', 'quarter', 'year',
            'weekday_revenue', 'weekend_revenue',
            'male_revenue', 'female_revenue'
        ]
        
        available_features = [col for col in feature_columns if col in df.columns]
        target_col = 'monthly_revenue'
        
        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        for col in available_features:
            if df[col].dtype in ['object']:
                df[col] = df[col].fillna('unknown')
                df[col] = pd.Categorical(df[col]).codes
            else:
                df[col] = df[col].fillna(df[col].median())
        
        df[target_col] = df[target_col].fillna(df[target_col].median())
        
        # ì´ìƒì¹˜ ì œê±°
        target_q99 = df[target_col].quantile(0.99)
        target_q01 = df[target_col].quantile(0.01)
        df = df[(df[target_col] >= target_q01) & (df[target_col] <= target_q99)]
        
        # í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ì¶”ì¶œ (ì „ì²´ ë°ì´í„°ì˜ 10%)
        test_sample = df.sample(n=min(10000, len(df) // 10), random_state=42)
        
        X_test = test_sample[available_features]
        y_test = test_sample[target_col]
        
        logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(X_test):,} ìƒ˜í”Œ")
        return X_test, y_test
    
    def evaluate_single_model(self, model, X_test: pd.DataFrame, y_test: pd.Series, 
                            model_name: str) -> Dict:
        """ë‹¨ì¼ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        try:
            # ì˜ˆì¸¡ ì‹¤í–‰
            start_time = time.time()
            y_pred = model.predict(X_test)
            prediction_time = time.time() - start_time
            
            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            mse = mean_squared_error(y_test, y_pred)
            mae = mean_absolute_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            rmse = np.sqrt(mse)
            
            # MAPE ê³„ì‚° (0 ê°’ ë•Œë¬¸ì— ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
            mask = y_test != 0
            mape = mean_absolute_percentage_error(y_test[mask], y_pred[mask]) if mask.sum() > 0 else np.inf
            
            # ì¶”ê°€ í†µê³„
            residuals = y_test - y_pred
            residual_std = np.std(residuals)
            
            return {
                'model_name': model_name,
                'mse': float(mse),
                'mae': float(mae),
                'r2': float(r2),
                'rmse': float(rmse),
                'mape': float(mape) if mape != np.inf else None,
                'residual_std': float(residual_std),
                'prediction_time': float(prediction_time),
                'predictions_per_second': float(len(X_test) / prediction_time) if prediction_time > 0 else 0,
                'sample_count': len(X_test)
            }
            
        except Exception as e:
            logger.warning(f"ëª¨ë¸ {model_name} í‰ê°€ ì‹¤íŒ¨: {e}")
            return {
                'model_name': model_name,
                'error': str(e),
                'success': False
            }
    
    def evaluate_all_models(self, models: Dict, X_test: pd.DataFrame, y_test: pd.Series) -> Dict:
        """ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        logger.info("ğŸ“Š ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
        
        results = {}
        
        # ê¸€ë¡œë²Œ ëª¨ë¸ í‰ê°€
        if 'global' in models:
            logger.info("   ê¸€ë¡œë²Œ ëª¨ë¸ í‰ê°€ ì¤‘...")
            global_result = self.evaluate_single_model(models['global'], X_test, y_test, 'global')
            results['global'] = global_result
        
        # ì§€ì—­ ëª¨ë¸ í‰ê°€
        if 'regional' in models:
            logger.info(f"   ì§€ì—­ ëª¨ë¸ {len(models['regional'])}ê°œ í‰ê°€ ì¤‘...")
            regional_results = {}
            
            for region_key, model in models['regional'].items():
                region_id = region_key.split('_')[1]
                
                # í•´ë‹¹ ì§€ì—­ ë°ì´í„°ë§Œ í•„í„°ë§
                region_mask = X_test['district_code'] == int(region_id)
                if region_mask.sum() < 10:  # ìµœì†Œ ìƒ˜í”Œ ìˆ˜ í™•ì¸
                    continue
                
                X_region = X_test[region_mask]
                y_region = y_test[region_mask]
                
                result = self.evaluate_single_model(model, X_region, y_region, region_key)
                regional_results[region_key] = result
            
            results['regional'] = regional_results
        
        # ë¡œì»¬ ëª¨ë¸ í‰ê°€ (ìƒ˜í”Œë§í•˜ì—¬ ì¼ë¶€ë§Œ)
        if 'local' in models:
            logger.info(f"   ë¡œì»¬ ëª¨ë¸ {len(models['local'])}ê°œ í‰ê°€ ì¤‘...")
            local_results = {}
            evaluated_count = 0
            max_local_evaluations = 10  # ì‹œê°„ ì ˆì•½ì„ ìœ„í•´ ì¼ë¶€ë§Œ í‰ê°€
            
            for local_key, model in models['local'].items():
                if evaluated_count >= max_local_evaluations:
                    break
                
                parts = local_key.split('_')
                region_id = int(parts[1])
                business_id = int(parts[2])
                
                # í•´ë‹¹ ì§€ì—­+ì—…ì¢… ë°ì´í„° í•„í„°ë§
                local_mask = (X_test['district_code'] == region_id) & (X_test['business_type_code'] == business_id)
                if local_mask.sum() < 5:  # ìµœì†Œ ìƒ˜í”Œ ìˆ˜ í™•ì¸
                    continue
                
                X_local = X_test[local_mask]
                y_local = y_test[local_mask]
                
                result = self.evaluate_single_model(model, X_local, y_local, local_key)
                local_results[local_key] = result
                evaluated_count += 1
            
            results['local'] = local_results
            logger.info(f"   ë¡œì»¬ ëª¨ë¸ {evaluated_count}ê°œ í‰ê°€ ì™„ë£Œ")
        
        self.evaluation_results['model_performance'] = results
        return results
    
    def analyze_performance_patterns(self, results: Dict) -> Dict:
        """ì„±ëŠ¥ íŒ¨í„´ ë¶„ì„"""
        logger.info("ğŸ” ì„±ëŠ¥ íŒ¨í„´ ë¶„ì„ ì¤‘...")
        
        analysis = {
            'model_tier_comparison': {},
            'performance_distribution': {},
            'speed_analysis': {},
            'best_worst_models': {}
        }
        
        # ëª¨ë¸ ê³„ì¸µë³„ ì„±ëŠ¥ ë¹„êµ
        tier_stats = {}
        
        for tier in ['global', 'regional', 'local']:
            if tier in results and results[tier]:
                if tier == 'global':
                    models_data = [results[tier]]
                else:
                    models_data = list(results[tier].values())
                
                # ì„±ê³µí•œ ëª¨ë¸ë“¤ë§Œ í•„í„°ë§
                successful_models = [m for m in models_data if 'error' not in m and 'r2' in m]
                
                if successful_models:
                    r2_scores = [m['r2'] for m in successful_models]
                    mae_scores = [m['mae'] for m in successful_models]
                    prediction_speeds = [m['predictions_per_second'] for m in successful_models]
                    
                    tier_stats[tier] = {
                        'model_count': len(successful_models),
                        'avg_r2': float(np.mean(r2_scores)),
                        'std_r2': float(np.std(r2_scores)),
                        'avg_mae': float(np.mean(mae_scores)),
                        'avg_speed': float(np.mean(prediction_speeds)),
                        'best_r2': float(max(r2_scores)),
                        'worst_r2': float(min(r2_scores))
                    }
        
        analysis['model_tier_comparison'] = tier_stats
        
        # ìµœê³ /ìµœì•… ëª¨ë¸ ì°¾ê¸°
        all_models = []
        for tier, tier_results in results.items():
            if tier == 'global':
                if 'r2' in tier_results:
                    all_models.append({**tier_results, 'tier': tier})
            else:
                for model_name, model_result in tier_results.items():
                    if 'r2' in model_result:
                        all_models.append({**model_result, 'tier': tier})
        
        if all_models:
            # R2 ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
            all_models.sort(key=lambda x: x['r2'], reverse=True)
            
            analysis['best_worst_models'] = {
                'best_3': all_models[:3],
                'worst_3': all_models[-3:] if len(all_models) >= 3 else all_models
            }
        
        self.evaluation_results['comparative_analysis'] = analysis
        return analysis
    
    def generate_recommendations(self, results: Dict, analysis: Dict) -> List[str]:
        """ì„±ëŠ¥ ë¶„ì„ ê¸°ë°˜ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        logger.info("ğŸ’¡ ì¶”ì²œì‚¬í•­ ìƒì„± ì¤‘...")
        
        recommendations = []
        
        # ëª¨ë¸ ê³„ì¸µë³„ ì„±ëŠ¥ ë¶„ì„
        tier_stats = analysis.get('model_tier_comparison', {})
        
        if 'global' in tier_stats and 'regional' in tier_stats:
            global_r2 = tier_stats['global']['avg_r2']
            regional_r2 = tier_stats['regional']['avg_r2']
            
            if regional_r2 > global_r2:
                diff = regional_r2 - global_r2
                recommendations.append(f"âœ… ì§€ì—­ ëª¨ë¸ì´ ê¸€ë¡œë²Œ ëª¨ë¸ë³´ë‹¤ {diff:.3f}ë§Œí¼ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ì§€ì—­ë³„ íŠ¹ì„±ì„ ì˜ ë°˜ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
            else:
                recommendations.append("âš ï¸  ê¸€ë¡œë²Œ ëª¨ë¸ì´ ì§€ì—­ ëª¨ë¸ê³¼ ë¹„ìŠ·í•˜ê±°ë‚˜ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ì§€ì—­ë³„ íŠ¹ì„± ë°˜ì˜ì„ ê°œì„ í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        if 'local' in tier_stats:
            local_r2_avg = tier_stats['local']['avg_r2']
            local_r2_std = tier_stats['local']['std_r2']
            
            if local_r2_std > 0.1:
                recommendations.append("âš ï¸  ë¡œì»¬ ëª¨ë¸ë“¤ ê°„ ì„±ëŠ¥ í¸ì°¨ê°€ í½ë‹ˆë‹¤. ë°ì´í„° ë¶€ì¡±í•œ ì¡°í•©ë“¤ì˜ ëª¨ë¸ì„ ê°œì„ í•˜ê±°ë‚˜ fallback ì „ëµì„ ê°•í™”í•˜ì„¸ìš”.")
            
            if local_r2_avg < 0.8:
                recommendations.append("ğŸ”§ ì¼ë¶€ ë¡œì»¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë‚®ìŠµë‹ˆë‹¤. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ì´ë‚˜ ëª¨ë¸ ë³µì¡ë„ë¥¼ ì¡°ì •í•´ë³´ì„¸ìš”.")
        
        # ì†ë„ ë¶„ì„
        if 'global' in tier_stats:
            global_speed = tier_stats['global']['avg_speed']
            if global_speed < 1000:  # ì´ˆë‹¹ 1000ê°œ ë¯¸ë§Œ
                recommendations.append("âš¡ ê¸€ë¡œë²Œ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì†ë„ê°€ ëŠë¦½ë‹ˆë‹¤. ëª¨ë¸ ìµœì í™”ë‚˜ ë” ê°„ë‹¨í•œ ì•Œê³ ë¦¬ì¦˜ ê³ ë ¤í•´ë³´ì„¸ìš”.")
        
        # ìµœê³ /ìµœì•… ëª¨ë¸ ë¶„ì„
        best_worst = analysis.get('best_worst_models', {})
        if 'worst_3' in best_worst:
            worst_models = best_worst['worst_3']
            worst_r2_avg = np.mean([m['r2'] for m in worst_models])
            
            if worst_r2_avg < 0.7:
                recommendations.append(f"ğŸš¨ ìµœí•˜ìœ„ ëª¨ë¸ë“¤ì˜ í‰ê·  R2ê°€ {worst_r2_avg:.3f}ë¡œ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤. í•´ë‹¹ ëª¨ë¸ë“¤ì„ ì¬í›ˆë ¨í•˜ê±°ë‚˜ ì œê±°ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
        
        # ì „ì²´ì ì¸ ê¶Œì¥ì‚¬í•­
        if len(recommendations) == 0:
            recommendations.append("ğŸ‰ ëª¨ë“  ëª¨ë¸ì´ ì–‘í˜¸í•œ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤! í˜„ì¬ ì„¤ì •ì„ ìœ ì§€í•˜ì„¸ìš”.")
        
        self.evaluation_results['recommendations'] = recommendations
        return recommendations
    
    def generate_evaluation_report(self, results: Dict, analysis: Dict, recommendations: List[str]) -> str:
        """ì¢…í•© í‰ê°€ ë³´ê³ ì„œ ìƒì„±"""
        logger.info("ğŸ“‹ ì¢…í•© í‰ê°€ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        # ì „ì²´ í†µê³„
        total_models = 0
        successful_evaluations = 0
        
        for tier, tier_results in results.items():
            if tier == 'global':
                if 'error' not in tier_results:
                    successful_evaluations += 1
                total_models += 1
            else:
                tier_total = len(tier_results)
                tier_success = sum(1 for r in tier_results.values() if 'error' not in r)
                total_models += tier_total
                successful_evaluations += tier_success
        
        success_rate = (successful_evaluations / total_models * 100) if total_models > 0 else 0
        
        report = f"""
ğŸ¢ ì„œìš¸ ì‹œì¥ ìœ„í—˜ë„ ML ì‹œìŠ¤í…œ - ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë³´ê³ ì„œ
{'='*65}

ğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½:
  â€¢ í‰ê°€ ëŒ€ìƒ ëª¨ë¸: {total_models}ê°œ
  â€¢ ì„±ê³µì  í‰ê°€: {successful_evaluations}ê°œ
  â€¢ í‰ê°€ ì„±ê³µë¥ : {success_rate:.1f}%
  â€¢ í‰ê°€ ì™„ë£Œ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ğŸ”§ ëª¨ë¸ ê³„ì¸µë³„ ì„±ëŠ¥:
"""
        
        tier_stats = analysis.get('model_tier_comparison', {})
        for tier, stats in tier_stats.items():
            tier_name = {'global': 'ê¸€ë¡œë²Œ', 'regional': 'ì§€ì—­', 'local': 'ë¡œì»¬'}[tier]
            report += f"""  ğŸ“ {tier_name} ëª¨ë¸:
    - ëª¨ë¸ ìˆ˜: {stats['model_count']}ê°œ
    - í‰ê·  R2: {stats['avg_r2']:.3f} (Â±{stats['std_r2']:.3f})
    - í‰ê·  MAE: {stats['avg_mae']:,.0f}
    - í‰ê·  ì†ë„: {stats['avg_speed']:,.0f} ì˜ˆì¸¡/ì´ˆ
    - ìµœê³  ì„±ëŠ¥: R2 {stats['best_r2']:.3f}
    - ìµœì € ì„±ëŠ¥: R2 {stats['worst_r2']:.3f}

"""
        
        # ìµœê³ /ìµœì•… ëª¨ë¸
        best_worst = analysis.get('best_worst_models', {})
        if 'best_3' in best_worst:
            report += "ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ TOP 3:\n"
            for i, model in enumerate(best_worst['best_3'], 1):
                tier_name = {'global': 'ê¸€ë¡œë²Œ', 'regional': 'ì§€ì—­', 'local': 'ë¡œì»¬'}[model['tier']]
                report += f"  {i}. {model['model_name']} ({tier_name}) - R2: {model['r2']:.3f}\n"
            report += "\n"
        
        if 'worst_3' in best_worst:
            report += "âš ï¸  ê°œì„  í•„ìš” ëª¨ë¸:\n"
            for i, model in enumerate(best_worst['worst_3'], 1):
                tier_name = {'global': 'ê¸€ë¡œë²Œ', 'regional': 'ì§€ì—­', 'local': 'ë¡œì»¬'}[model['tier']]
                report += f"  {i}. {model['model_name']} ({tier_name}) - R2: {model['r2']:.3f}\n"
            report += "\n"
        
        # ì¶”ì²œì‚¬í•­
        report += "ğŸ’¡ ì¶”ì²œì‚¬í•­:\n"
        for i, rec in enumerate(recommendations, 1):
            report += f"  {i}. {rec}\n"
        
        # ë³´ê³ ì„œ íŒŒì¼ ì €ì¥
        report_path = Path("model_evaluation_report.txt")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        # ìƒì„¸ ê²°ê³¼ JSON ì €ì¥
        detailed_results = {
            'timestamp': self.evaluation_results['timestamp'].isoformat(),
            'model_performance': results,
            'comparative_analysis': analysis,
            'recommendations': recommendations,
            'summary': {
                'total_models': total_models,
                'successful_evaluations': successful_evaluations,
                'success_rate': success_rate
            }
        }
        
        json_path = Path("detailed_evaluation_results.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, indent=2, ensure_ascii=False, default=str)
        
        return report
    
    def run_full_evaluation(self):
        """ì „ì²´ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("ğŸš€ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹œì‘")
        logger.info("="*65)
        
        try:
            # 1. ëª¨ë¸ ë¡œë“œ
            models = self.load_trained_models()
            
            if not models:
                raise ValueError("í‰ê°€í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            # 2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
            X_test, y_test = self.prepare_test_data()
            
            # 3. ëª¨ë“  ëª¨ë¸ í‰ê°€
            results = self.evaluate_all_models(models, X_test, y_test)
            
            # 4. ì„±ëŠ¥ íŒ¨í„´ ë¶„ì„
            analysis = self.analyze_performance_patterns(results)
            
            # 5. ì¶”ì²œì‚¬í•­ ìƒì„±
            recommendations = self.generate_recommendations(results, analysis)
            
            # 6. ì¢…í•© ë³´ê³ ì„œ ìƒì„±
            report = self.generate_evaluation_report(results, analysis, recommendations)
            logger.info("\n" + report)
            
            return {
                'results': results,
                'analysis': analysis,
                'recommendations': recommendations
            }
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ëª¨ë¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    evaluator = SeoulModelEvaluator()
    evaluation = evaluator.run_full_evaluation()
    
    print(f"\nğŸ¯ í‰ê°€ ì™„ë£Œ!")
    print(f"  ë¶„ì„ëœ ëª¨ë¸: {len(evaluation['results'])} ê³„ì¸µ")
    print(f"  ì¶”ì²œì‚¬í•­: {len(evaluation['recommendations'])}ê°œ")
    print(f"  ìƒì„¸ ê²°ê³¼: detailed_evaluation_results.json")
    
    return 0


if __name__ == "__main__":
    exit(main())