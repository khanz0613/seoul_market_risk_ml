#!/usr/bin/env python3
"""
Full-Scale ML System with Complete Dataset
==========================================

ğŸ”§ ë¬¸ì œì  í•´ê²°:
1. ì „ì²´ 40ë§Œê°œ+ ë°ì´í„° ì‚¬ìš© (10,000ê°œ â†’ 400,000ê°œ+)
2. ì‹¤ì œ ì§€ì—­/ì—…ì¢… ì¸ì½”ë”© ì™„ì „ í•´ê²°
3. ê³ ì„±ëŠ¥ ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸

Author: Seoul Market Risk ML - Full Scale Version
Date: 2025-09-17
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib
import os
from pathlib import Path
import json
import time
from typing import Dict, List, Tuple
import warnings

warnings.filterwarnings('ignore')

class FullScaleMLSystem:
    """ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê³ ì„±ëŠ¥ ML ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.model = None
        self.region_encoder = LabelEncoder()
        self.business_encoder = LabelEncoder()
        self.scaler = StandardScaler()

        # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
        self.model_dir = "full_scale_models"
        Path(self.model_dir).mkdir(exist_ok=True)

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.training_stats = {
            'total_samples': 0,
            'unique_regions': 0,
            'unique_businesses': 0,
            'training_time': 0,
            'accuracy': 0,
            'cross_val_score': 0
        }

    def load_complete_seoul_data(self) -> pd.DataFrame:
        """ì „ì²´ ì„œìš¸ ìƒê¶Œ ë°ì´í„° ë¡œë“œ (40ë§Œê°œ+)"""
        print("ğŸ“‚ Loading COMPLETE Seoul commercial data...")

        data_dir = "data/raw"
        csv_files = list(Path(data_dir).glob("*.csv"))

        if not csv_files:
            raise FileNotFoundError(f"No CSV files found in {data_dir}")

        print(f"Found {len(csv_files)} data files")

        # ëª¨ë“  íŒŒì¼ ë¡œë“œ ë° ê²°í•©
        all_dataframes = []
        total_rows = 0

        for csv_file in csv_files:
            print(f"  Loading: {csv_file.name}")
            df = pd.read_csv(csv_file, encoding='utf-8')
            all_dataframes.append(df)
            total_rows += len(df)
            print(f"    Rows: {len(df):,}")

        # ëª¨ë“  ë°ì´í„° ê²°í•©
        combined_df = pd.concat(all_dataframes, ignore_index=True)

        print(f"âœ… Total combined data: {len(combined_df):,} rows")
        print(f"âœ… Columns: {len(combined_df.columns)}")

        # ê¸°ë³¸ ì •ë³´ ì¶œë ¥
        print(f"\nğŸ“Š Data Overview:")
        print(f"   Unique regions: {combined_df['í–‰ì •ë™_ì½”ë“œ_ëª…'].nunique():,}")
        print(f"   Unique businesses: {combined_df['ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…'].nunique():,}")
        print(f"   Date range: {combined_df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].min()} - {combined_df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].max()}")

        return combined_df

    def create_comprehensive_training_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ í›ˆë ¨ ë°ì´í„° ìƒì„± (40ë§Œê°œ ì „ì²´ ì‚¬ìš©)"""
        print("ğŸ”§ Creating comprehensive training data from FULL dataset...")

        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        df = df.dropna(subset=['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡', 'í–‰ì •ë™_ì½”ë“œ_ëª…', 'ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…'])

        # ë§¤ì¶œì•¡ì´ 0ì´ê±°ë‚˜ ìŒìˆ˜ì¸ ê²½ìš° ì œê±°
        df = df[df['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'] > 0]

        print(f"âœ… After cleaning: {len(df):,} valid records")

        # í›ˆë ¨ ë°ì´í„° êµ¬ì¡° ìƒì„±
        training_data = []

        for idx, row in df.iterrows():
            if idx % 50000 == 0:
                print(f"  Processing: {idx:,}/{len(df):,} ({idx/len(df)*100:.1f}%)")

            # ì‹¤ì œ ë§¤ì¶œ ë°ì´í„° ì‚¬ìš©
            base_sales = float(row['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'])

            # ì‹¤ì œ ê±°ë˜ ê±´ìˆ˜ ê³ ë ¤
            transaction_count = float(row.get('ë‹¹ì›”_ë§¤ì¶œ_ê±´ìˆ˜', 1))
            avg_transaction = base_sales / max(transaction_count, 1)

            # ì›”ë§¤ì¶œ = ì‹¤ì œ ë°ì´í„°
            monthly_revenue = base_sales

            # ë¹„ìš© êµ¬ì¡° ì¶”ì • (ì—…ì¢…ë³„ ì°¨ë³„í™”)
            business_type = row['ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…']

            # ì—…ì¢…ë³„ ë¹„ìš© êµ¬ì¡°
            if 'ìŒì‹ì ' in business_type or 'ì¹´í˜' in business_type:
                labor_ratio = np.random.uniform(0.25, 0.35)  # 25-35%
                rent_ratio = np.random.uniform(0.15, 0.25)   # 15-25%
                material_ratio = np.random.uniform(0.30, 0.40)  # 30-40%
                other_ratio = np.random.uniform(0.05, 0.15)   # 5-15%
            elif 'ì†Œë§¤' in business_type:
                labor_ratio = np.random.uniform(0.15, 0.25)
                rent_ratio = np.random.uniform(0.20, 0.30)
                material_ratio = np.random.uniform(0.40, 0.50)
                other_ratio = np.random.uniform(0.05, 0.10)
            else:  # ì„œë¹„ìŠ¤ì—…
                labor_ratio = np.random.uniform(0.30, 0.45)
                rent_ratio = np.random.uniform(0.10, 0.20)
                material_ratio = np.random.uniform(0.10, 0.20)
                other_ratio = np.random.uniform(0.10, 0.20)

            # ì´ìì‚° ì¶”ì • (ë§¤ì¶œ ê¸°ë°˜)
            total_assets = monthly_revenue * np.random.uniform(4, 10)

            # ê°€ìš©ìì‚° ì¶”ì • (ì´ìì‚°ì˜ ì¼ë¶€)
            available_cash = total_assets * np.random.uniform(0.15, 0.35)

            training_data.append({
                'ì´ìì‚°': total_assets,
                'ì›”ë§¤ì¶œ': monthly_revenue,
                'ì¸ê±´ë¹„': monthly_revenue * labor_ratio,
                'ì„ëŒ€ë£Œ': monthly_revenue * rent_ratio,
                'ì‹ìì¬ë¹„': monthly_revenue * material_ratio,
                'ê¸°íƒ€ë¹„ìš©': monthly_revenue * other_ratio,
                'ê°€ìš©ìì‚°': available_cash,
                'ì§€ì—­': row['í–‰ì •ë™_ì½”ë“œ_ëª…'],
                'ì—…ì¢…': row['ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…'],
                # ì¶”ê°€ ì •ë³´ (ë¶„ì„ìš©)
                'ê±°ë˜ê±´ìˆ˜': transaction_count,
                'í‰ê· ê±°ë˜ì•¡': avg_transaction,
                'ì—°ë„ë¶„ê¸°': row['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ']
            })

        training_df = pd.DataFrame(training_data)

        print(f"âœ… Complete training data created: {len(training_df):,} samples")
        print(f"   Unique regions: {training_df['ì§€ì—­'].nunique():,}")
        print(f"   Unique businesses: {training_df['ì—…ì¢…'].nunique():,}")

        # í†µê³„ ì—…ë°ì´íŠ¸
        self.training_stats['total_samples'] = len(training_df)
        self.training_stats['unique_regions'] = training_df['ì§€ì—­'].nunique()
        self.training_stats['unique_businesses'] = training_df['ì—…ì¢…'].nunique()

        return training_df

    def create_enhanced_labels(self, df: pd.DataFrame) -> pd.DataFrame:
        """í–¥ìƒëœ ë¼ë²¨ë§ ì‹œìŠ¤í…œ"""
        print("ğŸ·ï¸ Creating enhanced risk labels...")

        # ì¬ë¬´ ë¹„ìœ¨ ê³„ì‚°
        df['ì›”ë¹„ìš©'] = df['ì¸ê±´ë¹„'] + df['ì„ëŒ€ë£Œ'] + df['ì‹ìì¬ë¹„'] + df['ê¸°íƒ€ë¹„ìš©']
        df['ì›”ìˆœìµ'] = df['ì›”ë§¤ì¶œ'] - df['ì›”ë¹„ìš©']
        df['ìˆœìµë¥ '] = df['ì›”ìˆœìµ'] / df['ì›”ë§¤ì¶œ']
        df['ìì‚°íšŒì „ìœ¨'] = (df['ì›”ë§¤ì¶œ'] * 12) / df['ì´ìì‚°']
        df['í˜„ê¸ˆë¹„ìœ¨'] = df['ê°€ìš©ìì‚°'] / df['ì›”ë¹„ìš©']

        # Altman Z-Score ê°œì„  ë²„ì „
        working_capital = df['ê°€ìš©ìì‚°']
        retained_earnings = df['ì›”ìˆœìµ'] * 12 * 0.3  # ì‹¤ì œ ì´ìµ ê¸°ë°˜
        ebit = df['ì›”ìˆœìµ'] * 12
        market_value_equity = df['ì´ìì‚°'] - (df['ì´ìì‚°'] * 0.3)
        sales = df['ì›”ë§¤ì¶œ'] * 12

        # ì•ˆì „í•œ ë¶„ëª¨ ê³„ì‚°
        safe_assets = np.maximum(df['ì´ìì‚°'], 1000000)
        safe_debt = np.maximum(df['ì´ìì‚°'] * 0.3, 100000)

        # Z-Score ê³„ì‚°
        A = working_capital / safe_assets
        B = retained_earnings / safe_assets
        C = ebit / safe_assets
        D = market_value_equity / safe_debt
        E = sales / safe_assets

        df['z_score'] = 1.2*A + 1.4*B + 3.3*C + 0.6*D + 1.0*E

        # ê°œì„ ëœ ìœ„í—˜ë„ ë¶„ë¥˜ (ë” ì„¸ë°€í•œ êµ¬ë¶„)
        def assign_enhanced_risk_label(row):
            z_score = row['z_score']
            profit_margin = row['ìˆœìµë¥ ']
            cash_ratio = row['í˜„ê¸ˆë¹„ìœ¨']

            # ë³µí•© ì§€í‘œ ê³ ë ¤
            if z_score > 3.0 and profit_margin > 0.1 and cash_ratio > 2.0:
                return 1  # ë§¤ìš° ì•ˆì „
            elif z_score > 2.7 and profit_margin > 0.05:
                return 2  # ì•ˆì „
            elif z_score > 1.8 and profit_margin > 0:
                return 3  # ë³´í†µ
            elif z_score > 1.1 or profit_margin > -0.1:
                return 4  # ìœ„í—˜
            else:
                return 5  # ë§¤ìš° ìœ„í—˜

        df['risk_label'] = df.apply(assign_enhanced_risk_label, axis=1)

        print("âœ… Enhanced risk labels created:")
        print(df['risk_label'].value_counts().sort_index())

        return df

    def prepare_full_features(self, df: pd.DataFrame):
        """ì „ì²´ ë°ì´í„°ë¥¼ ìœ„í•œ ì™„ì „í•œ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§"""
        print("ğŸ”§ Preparing complete features for full dataset...")

        # ê¸°ë³¸ í”¼ì²˜
        total_cost = df['ì¸ê±´ë¹„'] + df['ì„ëŒ€ë£Œ'] + df['ì‹ìì¬ë¹„'] + df['ê¸°íƒ€ë¹„ìš©']

        features = [
            np.log1p(df['ì´ìì‚°']),      # ë¡œê·¸ ë³€í™˜
            np.log1p(df['ì›”ë§¤ì¶œ']),      # ë¡œê·¸ ë³€í™˜
            np.log1p(total_cost),        # ë¡œê·¸ ë³€í™˜
            np.log1p(df['ê°€ìš©ìì‚°']),    # ì¶”ê°€ í”¼ì²˜
            df['ì›”ë§¤ì¶œ'] / df['ì´ìì‚°'],  # ìì‚° íš¨ìœ¨ì„±
            total_cost / df['ì›”ë§¤ì¶œ'],   # ë¹„ìš© ë¹„ìœ¨
        ]

        # ì§€ì—­ ì¸ì½”ë”© (ì „ì²´ ë°ì´í„°)
        print(f"  Encoding {df['ì§€ì—­'].nunique():,} unique regions...")
        region_encoded = self.region_encoder.fit_transform(df['ì§€ì—­'])
        features.append(region_encoded)

        # ì—…ì¢… ì¸ì½”ë”© (ì „ì²´ ë°ì´í„°)
        print(f"  Encoding {df['ì—…ì¢…'].nunique():,} unique business types...")
        business_encoded = self.business_encoder.fit_transform(df['ì—…ì¢…'])
        features.append(business_encoded)

        # ì¶”ê°€ íŒŒìƒ í”¼ì²˜
        features.extend([
            df['ê±°ë˜ê±´ìˆ˜'],              # ê±°ë˜ ë¹ˆë„
            np.log1p(df['í‰ê· ê±°ë˜ì•¡']),   # ê±°ë˜ ê·œëª¨
        ])

        # í”¼ì²˜ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
        X = np.column_stack(features)

        # í‘œì¤€í™”
        X_scaled = self.scaler.fit_transform(X)

        print(f"âœ… Feature matrix: {X_scaled.shape}")
        print(f"   Features: 10ê°œ (ê¸°ë³¸ 6ê°œ + ì§€ì—­ + ì—…ì¢… + ì¶”ê°€ 2ê°œ)")

        return X_scaled

    def train_full_scale_model(self, X, y):
        """ì „ì²´ ë°ì´í„°ë¥¼ í™œìš©í•œ ê³ ì„±ëŠ¥ ML í›ˆë ¨"""
        print("ğŸ¤– Training FULL-SCALE ML model...")
        print(f"ğŸ“Š Training on {len(X):,} samples")

        start_time = time.time()

        # ë°ì´í„° ë¶„í•  (ë” í° ë°ì´í„°ì…‹ì´ë¯€ë¡œ ë” ì—„ê²©í•œ ë¶„í• )
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        print(f"ğŸ“Š Training samples: {len(X_train):,}")
        print(f"ğŸ“Š Test samples: {len(X_test):,}")

        # ê³ ì„±ëŠ¥ RandomForest ì„¤ì •
        self.model = RandomForestClassifier(
            n_estimators=200,        # ë” ë§ì€ íŠ¸ë¦¬
            max_depth=15,           # ë” ê¹Šì€ í•™ìŠµ
            min_samples_split=10,   # ë” ì„¸ë°€í•œ ë¶„í• 
            min_samples_leaf=5,     # ë” ì„¸ë°€í•œ ì
            max_features='sqrt',    # í”¼ì²˜ ì„œë¸Œìƒ˜í”Œë§
            random_state=42,
            class_weight='balanced',
            n_jobs=-1              # ë³‘ë ¬ ì²˜ë¦¬
        )

        # ì‹¤ì œ í›ˆë ¨
        print("ğŸ”„ Training in progress...")
        self.model.fit(X_train, y_train)

        training_time = time.time() - start_time
        self.training_stats['training_time'] = training_time

        # ì„±ëŠ¥ í‰ê°€
        y_pred = self.model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        self.training_stats['accuracy'] = accuracy

        print(f"âœ… Full-scale model trained successfully!")
        print(f"   Training time: {training_time:.1f} seconds")
        print(f"   Accuracy: {accuracy:.4f}")
        print(f"   Training samples: {len(X_train):,}")
        print(f"   Test samples: {len(X_test):,}")

        # êµì°¨ ê²€ì¦
        print("ğŸ”„ Performing cross-validation...")
        cv_scores = cross_val_score(self.model, X_train, y_train, cv=5, n_jobs=-1)
        cv_mean = cv_scores.mean()
        self.training_stats['cross_val_score'] = cv_mean

        print(f"ğŸ“Š 5-Fold CV Score: {cv_mean:.4f} (Â±{cv_scores.std():.4f})")

        # ìƒì„¸ ì„±ëŠ¥ ë¦¬í¬íŠ¸
        print("\nğŸ“Š Detailed Classification Report:")
        print(classification_report(y_test, y_pred))

        # í˜¼ë™ í–‰ë ¬
        print("\nğŸ“Š Confusion Matrix:")
        cm = confusion_matrix(y_test, y_pred)
        print(cm)

        # í”¼ì²˜ ì¤‘ìš”ë„
        feature_names = ['ì´ìì‚°', 'ì›”ë§¤ì¶œ', 'ì›”ë¹„ìš©', 'ê°€ìš©ìì‚°', 'ìì‚°íš¨ìœ¨ì„±',
                        'ë¹„ìš©ë¹„ìœ¨', 'ì§€ì—­', 'ì—…ì¢…', 'ê±°ë˜ê±´ìˆ˜', 'í‰ê· ê±°ë˜ì•¡']
        importances = self.model.feature_importances_

        print("\nğŸ“Š Feature Importance:")
        for name, importance in zip(feature_names, importances):
            print(f"   {name}: {importance:.3f}")

        return accuracy

    def save_full_model(self):
        """ì™„ì „í•œ ëª¨ë¸ ì €ì¥"""
        model_path = f"{self.model_dir}/full_scale_ml_model.joblib"
        encoder_path = f"{self.model_dir}/full_scale_encoders.joblib"
        scaler_path = f"{self.model_dir}/full_scale_scaler.joblib"
        stats_path = f"{self.model_dir}/training_stats.json"

        # ëª¨ë¸ ì €ì¥
        joblib.dump(self.model, model_path)

        # ì¸ì½”ë” ì €ì¥
        encoders = {
            'region_encoder': self.region_encoder,
            'business_encoder': self.business_encoder
        }
        joblib.dump(encoders, encoder_path)

        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        joblib.dump(self.scaler, scaler_path)

        # í†µê³„ ì €ì¥
        with open(stats_path, 'w') as f:
            json.dump(self.training_stats, f, indent=2)

        print(f"âœ… Full-scale model saved:")
        print(f"   Model: {model_path}")
        print(f"   Encoders: {encoder_path}")
        print(f"   Scaler: {scaler_path}")
        print(f"   Stats: {stats_path}")

    def predict_with_full_model(self, ì´ìì‚°, ì›”ë§¤ì¶œ, ì¸ê±´ë¹„, ì„ëŒ€ë£Œ, ì‹ìì¬ë¹„,
                               ê¸°íƒ€ë¹„ìš©, ê°€ìš©ìì‚°, ì§€ì—­, ì—…ì¢…):
        """ì™„ì „í•œ ëª¨ë¸ë¡œ ì˜ˆì¸¡"""
        if self.model is None:
            raise ValueError("Model not trained!")

        print("ğŸ¯ Full-Scale ML Prediction")
        print("=" * 40)

        # í”¼ì²˜ ì¤€ë¹„
        total_cost = ì¸ê±´ë¹„ + ì„ëŒ€ë£Œ + ì‹ìì¬ë¹„ + ê¸°íƒ€ë¹„ìš©

        # ì§€ì—­/ì—…ì¢… ì¸ì½”ë”© (í›ˆë ¨ëœ ì¸ì½”ë” ì‚¬ìš©)
        try:
            region_encoded = self.region_encoder.transform([ì§€ì—­])[0]
            print(f"âœ… Region '{ì§€ì—­}' recognized")
        except ValueError:
            region_encoded = 0
            print(f"âš ï¸ Unknown region '{ì§€ì—­}', using fallback")

        try:
            business_encoded = self.business_encoder.transform([ì—…ì¢…])[0]
            print(f"âœ… Business '{ì—…ì¢…}' recognized")
        except ValueError:
            business_encoded = 0
            print(f"âš ï¸ Unknown business '{ì—…ì¢…}', using fallback")

        # í”¼ì²˜ ë²¡í„° (í›ˆë ¨ ì‹œì™€ ë™ì¼í•œ êµ¬ì¡°)
        features = np.array([
            np.log1p(ì´ìì‚°),
            np.log1p(ì›”ë§¤ì¶œ),
            np.log1p(total_cost),
            np.log1p(ê°€ìš©ìì‚°),
            ì›”ë§¤ì¶œ / ì´ìì‚°,        # ìì‚° íš¨ìœ¨ì„±
            total_cost / ì›”ë§¤ì¶œ,    # ë¹„ìš© ë¹„ìœ¨
            region_encoded,
            business_encoded,
            100,                    # ê¸°ë³¸ ê±°ë˜ê±´ìˆ˜
            ì›”ë§¤ì¶œ / 100           # í‰ê·  ê±°ë˜ì•¡
        ]).reshape(1, -1)

        # í‘œì¤€í™”
        features_scaled = self.scaler.transform(features)

        # ì˜ˆì¸¡
        risk_level = self.model.predict(features_scaled)[0]
        confidence = max(self.model.predict_proba(features_scaled)[0]) * 100

        risk_names = {1: "ë§¤ìš°ì•ˆì „", 2: "ì•ˆì „", 3: "ë³´í†µ", 4: "ìœ„í—˜", 5: "ë§¤ìš°ìœ„í—˜"}

        print(f"\nğŸ¯ Full-Scale ML Result:")
        print(f"   Risk Level: {risk_level} ({risk_names[risk_level]})")
        print(f"   Confidence: {confidence:.1f}%")
        print(f"   Model: Full-Scale RandomForest")
        print(f"   Training Data: {self.training_stats['total_samples']:,} samples")

        return {
            'risk_level': risk_level,
            'risk_name': risk_names[risk_level],
            'confidence': confidence,
            'model_type': 'Full-Scale ML (400K+ samples)',
            'training_stats': self.training_stats
        }

def main():
    """ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ML ì‹œìŠ¤í…œ ì‹¤í–‰"""
    print("ğŸš€ Full-Scale ML System (400K+ Samples)")
    print("=" * 60)
    print("ğŸ¯ Goal: Use COMPLETE dataset for maximum performance")
    print("ğŸ”§ Fix: Unknown region/business encoding issues")

    try:
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        ml_system = FullScaleMLSystem()

        # 1. ì „ì²´ ë°ì´í„° ë¡œë“œ
        raw_data = ml_system.load_complete_seoul_data()

        # 2. í›ˆë ¨ ë°ì´í„° ìƒì„± (ì „ì²´ ì‚¬ìš©)
        training_data = ml_system.create_comprehensive_training_data(raw_data)

        # 3. ë¼ë²¨ë§
        labeled_data = ml_system.create_enhanced_labels(training_data)

        # 4. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
        X = ml_system.prepare_full_features(labeled_data)
        y = labeled_data['risk_label'].values

        # 5. ëª¨ë¸ í›ˆë ¨
        accuracy = ml_system.train_full_scale_model(X, y)

        # 6. ëª¨ë¸ ì €ì¥
        ml_system.save_full_model()

        print("\n" + "=" * 70)
        print("âœ… FULL-SCALE ML SYSTEM READY!")
        print("ğŸ‰ Complete Dataset Training Successful")
        print(f"ğŸ“Š Final Accuracy: {accuracy:.4f}")
        print(f"ğŸ“Š Training Samples: {ml_system.training_stats['total_samples']:,}")
        print(f"ğŸ“Š Unique Regions: {ml_system.training_stats['unique_regions']:,}")
        print(f"ğŸ“Š Unique Businesses: {ml_system.training_stats['unique_businesses']:,}")
        print("=" * 70)

        # 7. í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
        print("\nğŸ§ª Testing full-scale prediction...")
        result = ml_system.predict_with_full_model(
            ì´ìì‚°=50000000,      # 5ì²œë§Œì›
            ì›”ë§¤ì¶œ=12000000,      # 1200ë§Œì›
            ì¸ê±´ë¹„=3000000,       # 300ë§Œì›
            ì„ëŒ€ë£Œ=2000000,       # 200ë§Œì›
            ì‹ìì¬ë¹„=3500000,     # 350ë§Œì›
            ê¸°íƒ€ë¹„ìš©=500000,      # 50ë§Œì›
            ê°€ìš©ìì‚°=15000000,    # 1500ë§Œì›
            ì§€ì—­='ê°•ë‚¨êµ¬',         # ì‹¤ì œ ì§€ì—­ í…ŒìŠ¤íŠ¸
            ì—…ì¢…='ì»¤í”¼ì „ë¬¸ì '      # ì‹¤ì œ ì—…ì¢… í…ŒìŠ¤íŠ¸
        )

        print(f"\nğŸ¯ Test Result: {result['risk_name']} ({result['confidence']:.1f}%)")
        print(f"âœ… FULL-SCALE SYSTEM TEST COMPLETE!")

    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()