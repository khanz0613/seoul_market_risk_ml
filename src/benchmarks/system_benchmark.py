#!/usr/bin/env python3
"""
ì„œìš¸ ì‹œì¥ ìœ„í—˜ë„ ML ì‹œìŠ¤í…œ - ìµœì¢… ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬
Seoul Market Risk ML System - Final System Benchmark

ì „ì²´ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥, ì•ˆì •ì„±, ë°°í¬ ì¤€ë¹„ ìƒíƒœë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import sys
import os
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, 'src'))

import pandas as pd
import numpy as np
import json
from datetime import datetime
from pathlib import Path
import time
import logging
import warnings
from typing import Dict, List, Tuple, Optional
import psutil
import gc

# System imports
from src.utils.config_loader import load_config, get_data_paths

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SystemBenchmark:
    """ì‹œìŠ¤í…œ ì¢…í•© ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.config = load_config()
        self.data_paths = get_data_paths(self.config)
        self.start_time = datetime.now()
        
        self.benchmark_results = {
            'timestamp': self.start_time,
            'system_info': {},
            'data_validation': {},
            'model_inventory': {},
            'performance_benchmarks': {},
            'resource_usage': {},
            'deployment_readiness': {},
            'final_recommendations': []
        }
        
    def collect_system_info(self):
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        logger.info("ğŸ’» ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        
        try:
            system_info = {
                'python_version': sys.version,
                'cpu_count': psutil.cpu_count(),
                'cpu_freq': psutil.cpu_freq()._asdict() if psutil.cpu_freq() else None,
                'memory': psutil.virtual_memory()._asdict(),
                'disk_usage': psutil.disk_usage('/')._asdict(),
                'platform': os.name,
                'working_directory': os.getcwd(),
                'project_structure': self._analyze_project_structure()
            }
            
            self.benchmark_results['system_info'] = system_info
            
            # ë©”ëª¨ë¦¬ ì •ë³´ ë¡œê¹…
            memory_gb = system_info['memory']['total'] / (1024**3)
            available_gb = system_info['memory']['available'] / (1024**3)
            
            logger.info(f"   CPU: {system_info['cpu_count']}ì½”ì–´")
            logger.info(f"   ë©”ëª¨ë¦¬: {memory_gb:.1f}GB ì „ì²´, {available_gb:.1f}GB ì‚¬ìš© ê°€ëŠ¥")
            
        except Exception as e:
            logger.warning(f"ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    def _analyze_project_structure(self) -> Dict:
        """í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„"""
        structure = {}
        
        important_dirs = [
            'src', 'config', 'data/processed', 'models', 
            'test_outputs', 'logs'
        ]
        
        for dir_name in important_dirs:
            dir_path = Path(dir_name)
            if dir_path.exists():
                if dir_path.is_dir():
                    file_count = len(list(dir_path.rglob('*')))
                    structure[dir_name] = {'exists': True, 'file_count': file_count}
                else:
                    structure[dir_name] = {'exists': True, 'type': 'file'}
            else:
                structure[dir_name] = {'exists': False}
        
        return structure
    
    def validate_data_integrity(self):
        """ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦"""
        logger.info("ğŸ“Š ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ì¤‘...")
        
        validation_results = {
            'processed_data_exists': False,
            'data_quality_metrics': {},
            'data_consistency': {}
        }
        
        try:
            # ì²˜ë¦¬ëœ ë°ì´í„° í™•ì¸
            combined_file = self.data_paths['processed'] / 'seoul_sales_combined.csv'
            
            if combined_file.exists():
                validation_results['processed_data_exists'] = True
                
                # ë°ì´í„° í’ˆì§ˆ ë©”íŠ¸ë¦­
                df = pd.read_csv(combined_file, nrows=10000)  # ìƒ˜í”Œ í™•ì¸
                
                validation_results['data_quality_metrics'] = {
                    'total_rows': len(df),
                    'total_columns': len(df.columns),
                    'missing_values_pct': (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100,
                    'duplicate_rows': df.duplicated().sum(),
                    'numeric_columns': len(df.select_dtypes(include=[np.number]).columns),
                    'categorical_columns': len(df.select_dtypes(include=['object']).columns)
                }
                
                # ì£¼ìš” ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
                required_columns = ['monthly_revenue', 'district_code', 'business_type_code']
                missing_columns = [col for col in required_columns if col not in df.columns]
                
                validation_results['data_consistency'] = {
                    'required_columns_present': len(missing_columns) == 0,
                    'missing_columns': missing_columns,
                    'revenue_stats': df['monthly_revenue'].describe().to_dict() if 'monthly_revenue' in df.columns else None
                }
                
                logger.info(f"   âœ… ë°ì´í„° íŒŒì¼ ì¡´ì¬: {len(df):,} í–‰")
                logger.info(f"   ê²°ì¸¡ê°’ ë¹„ìœ¨: {validation_results['data_quality_metrics']['missing_values_pct']:.2f}%")
                
            else:
                logger.warning("   âŒ ì²˜ë¦¬ëœ ë°ì´í„° íŒŒì¼ì´ ì—†ìŒ")
                
        except Exception as e:
            logger.error(f"ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
            validation_results['error'] = str(e)
        
        self.benchmark_results['data_validation'] = validation_results
    
    def inventory_models(self):
        """ëª¨ë¸ ì¸ë²¤í† ë¦¬ ë° ìƒíƒœ í™•ì¸"""
        logger.info("ğŸ¤– ëª¨ë¸ ì¸ë²¤í† ë¦¬ í™•ì¸ ì¤‘...")
        
        models_dir = Path("models")
        inventory = {
            'models_directory_exists': models_dir.exists(),
            'global_models': [],
            'regional_models': [],
            'local_models': [],
            'total_model_count': 0,
            'total_model_size_mb': 0
        }
        
        if models_dir.exists():
            # ëª¨ë¸ íŒŒì¼ë“¤ íƒìƒ‰
            for model_file in models_dir.glob('*.joblib'):
                file_size_mb = model_file.stat().st_size / (1024 * 1024)
                
                model_info = {
                    'filename': model_file.name,
                    'size_mb': round(file_size_mb, 2),
                    'modified': datetime.fromtimestamp(model_file.stat().st_mtime).isoformat()
                }
                
                if 'global' in model_file.name:
                    inventory['global_models'].append(model_info)
                elif 'regional' in model_file.name:
                    inventory['regional_models'].append(model_info)
                elif 'local' in model_file.name:
                    inventory['local_models'].append(model_info)
                
                inventory['total_model_size_mb'] += file_size_mb
                inventory['total_model_count'] += 1
            
            logger.info(f"   ê¸€ë¡œë²Œ ëª¨ë¸: {len(inventory['global_models'])}ê°œ")
            logger.info(f"   ì§€ì—­ ëª¨ë¸: {len(inventory['regional_models'])}ê°œ")
            logger.info(f"   ë¡œì»¬ ëª¨ë¸: {len(inventory['local_models'])}ê°œ")
            logger.info(f"   ì´ ëª¨ë¸ í¬ê¸°: {inventory['total_model_size_mb']:.1f}MB")
            
        else:
            logger.warning("   âŒ ëª¨ë¸ ë””ë ‰í† ë¦¬ê°€ ì—†ìŒ")
        
        self.benchmark_results['model_inventory'] = inventory
    
    def run_performance_benchmarks(self):
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        logger.info("âš¡ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘...")
        
        benchmarks = {
            'data_loading_speed': {},
            'model_loading_speed': {},
            'prediction_throughput': {},
            'memory_efficiency': {}
        }
        
        try:
            # ë°ì´í„° ë¡œë”© ì†ë„ í…ŒìŠ¤íŠ¸
            logger.info("   ë°ì´í„° ë¡œë”© ì†ë„ ì¸¡ì •...")
            start_time = time.time()
            
            combined_file = self.data_paths['processed'] / 'seoul_sales_combined.csv'
            if combined_file.exists():
                df = pd.read_csv(combined_file, nrows=50000)  # 5ë§Œ í–‰ í…ŒìŠ¤íŠ¸
                loading_time = time.time() - start_time
                
                benchmarks['data_loading_speed'] = {
                    'rows_loaded': len(df),
                    'loading_time_seconds': loading_time,
                    'rows_per_second': len(df) / loading_time if loading_time > 0 else 0
                }
                
            # ëª¨ë¸ ë¡œë”© ì†ë„ í…ŒìŠ¤íŠ¸ (ìƒ˜í”Œ)
            logger.info("   ëª¨ë¸ ë¡œë”© ì†ë„ ì¸¡ì •...")
            models_dir = Path("models")
            
            if models_dir.exists():
                model_files = list(models_dir.glob('*.joblib'))[:3]  # ìƒ˜í”Œ 3ê°œ
                loading_times = []
                
                for model_file in model_files:
                    start_time = time.time()
                    try:
                        import joblib
                        _ = joblib.load(model_file)
                        loading_time = time.time() - start_time
                        loading_times.append(loading_time)
                    except:
                        pass
                
                if loading_times:
                    benchmarks['model_loading_speed'] = {
                        'models_tested': len(loading_times),
                        'avg_loading_time': np.mean(loading_times),
                        'total_time': sum(loading_times)
                    }
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
            process = psutil.Process()
            memory_info = process.memory_info()
            
            benchmarks['memory_efficiency'] = {
                'current_memory_mb': memory_info.rss / (1024 * 1024),
                'peak_memory_mb': memory_info.vms / (1024 * 1024),
                'memory_percent': process.memory_percent()
            }
            
            logger.info(f"   í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {benchmarks['memory_efficiency']['current_memory_mb']:.1f}MB")
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
            benchmarks['error'] = str(e)
        
        self.benchmark_results['performance_benchmarks'] = benchmarks
    
    def assess_deployment_readiness(self):
        """ë°°í¬ ì¤€ë¹„ ìƒíƒœ í‰ê°€"""
        logger.info("ğŸš€ ë°°í¬ ì¤€ë¹„ ìƒíƒœ í‰ê°€ ì¤‘...")
        
        readiness = {
            'configuration_status': {},
            'required_files_check': {},
            'dependencies_check': {},
            'security_check': {},
            'scalability_assessment': {},
            'overall_readiness_score': 0
        }
        
        score = 0
        max_score = 100
        
        # 1. ì„¤ì • íŒŒì¼ ìƒíƒœ (20ì )
        config_file = Path("config/config.yaml")
        if config_file.exists():
            readiness['configuration_status'] = {
                'config_file_exists': True,
                'config_valid': True  # ì´ë¯¸ ë¡œë“œë˜ì—ˆìœ¼ë¯€ë¡œ ìœ íš¨
            }
            score += 20
        
        # 2. í•„ìˆ˜ íŒŒì¼ í™•ì¸ (25ì )
        essential_files = [
            'src/models/global_model.py',
            'src/risk_scoring/risk_calculator.py',
            'src/loan_calculation/loan_calculator.py',
            'models/global_model.joblib'
        ]
        
        existing_files = [f for f in essential_files if Path(f).exists()]
        file_score = (len(existing_files) / len(essential_files)) * 25
        score += file_score
        
        readiness['required_files_check'] = {
            'required_files': essential_files,
            'existing_files': existing_files,
            'completion_rate': len(existing_files) / len(essential_files)
        }
        
        # 3. ì˜ì¡´ì„± í™•ì¸ (20ì )
        try:
            import pandas, numpy, sklearn, joblib
            dependencies_ok = True
            score += 20
        except ImportError as e:
            dependencies_ok = False
            readiness['dependencies_check']['missing'] = str(e)
        
        readiness['dependencies_check'] = {
            'core_dependencies_available': dependencies_ok,
            'import_test_passed': dependencies_ok
        }
        
        # 4. ë³´ì•ˆ ì²´í¬ (15ì ) - ê¸°ë³¸ì ì¸ í™•ì¸
        secret_files = ['.env', 'secrets.yaml', 'api_keys.txt']
        exposed_secrets = [f for f in secret_files if Path(f).exists()]
        
        if not exposed_secrets:
            score += 15
            readiness['security_check'] = {
                'no_exposed_secrets': True,
                'basic_security_ok': True
            }
        else:
            readiness['security_check'] = {
                'no_exposed_secrets': False,
                'exposed_files': exposed_secrets
            }
        
        # 5. í™•ì¥ì„± í‰ê°€ (20ì )
        models_count = self.benchmark_results.get('model_inventory', {}).get('total_model_count', 0)
        data_size = self.benchmark_results.get('data_validation', {}).get('data_quality_metrics', {}).get('total_rows', 0)
        
        scalability_score = 0
        if models_count > 20:
            scalability_score += 10
        if data_size > 100000:
            scalability_score += 10
        
        score += scalability_score
        
        readiness['scalability_assessment'] = {
            'model_count': models_count,
            'data_scale': data_size,
            'scalability_ready': scalability_score >= 15
        }
        
        readiness['overall_readiness_score'] = score
        
        # ë°°í¬ ê¶Œì¥ì‚¬í•­
        if score >= 90:
            readiness_level = "ğŸŸ¢ ë°°í¬ ì¤€ë¹„ ì™„ë£Œ"
        elif score >= 70:
            readiness_level = "ğŸŸ¡ ë°°í¬ ê°€ëŠ¥ (ì¼ë¶€ ê°œì„  ê¶Œì¥)"
        else:
            readiness_level = "ğŸ”´ ë°°í¬ ì „ ê°œì„  í•„ìš”"
        
        readiness['deployment_recommendation'] = readiness_level
        
        logger.info(f"   ë°°í¬ ì¤€ë¹„ ì ìˆ˜: {score}/100ì ")
        logger.info(f"   ìƒíƒœ: {readiness_level}")
        
        self.benchmark_results['deployment_readiness'] = readiness
    
    def generate_final_recommendations(self):
        """ìµœì¢… ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        logger.info("ğŸ’¡ ìµœì¢… ê¶Œì¥ì‚¬í•­ ìƒì„± ì¤‘...")
        
        recommendations = []
        
        # ë°°í¬ ì¤€ë¹„ë„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        deployment_score = self.benchmark_results.get('deployment_readiness', {}).get('overall_readiness_score', 0)
        
        if deployment_score >= 90:
            recommendations.append("ğŸ‰ ì‹œìŠ¤í…œì´ í”„ë¡œë•ì…˜ ë°°í¬ ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        elif deployment_score >= 70:
            recommendations.append("âš ï¸  ì¼ë¶€ ê°œì„  í›„ ë°°í¬ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
        else:
            recommendations.append("ğŸ”§ ë°°í¬ ì „ ì£¼ìš” ì´ìŠˆë“¤ì„ í•´ê²°í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        # ì„±ëŠ¥ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        memory_usage = self.benchmark_results.get('performance_benchmarks', {}).get('memory_efficiency', {}).get('current_memory_mb', 0)
        
        if memory_usage > 1000:  # 1GB ì´ˆê³¼
            recommendations.append(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤ ({memory_usage:.0f}MB). ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
        
        # ëª¨ë¸ ê´€ë ¨ ê¶Œì¥ì‚¬í•­
        model_count = self.benchmark_results.get('model_inventory', {}).get('total_model_count', 0)
        
        if model_count > 0:
            recommendations.append(f"âœ… {model_count}ê°œ ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            if model_count < 20:
                recommendations.append("ğŸ“ˆ ë” ë§ì€ ë¡œì»¬ ëª¨ë¸ì„ ìƒì„±í•˜ì—¬ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # ë°ì´í„° ê´€ë ¨ ê¶Œì¥ì‚¬í•­
        data_validation = self.benchmark_results.get('data_validation', {})
        if data_validation.get('processed_data_exists', False):
            missing_pct = data_validation.get('data_quality_metrics', {}).get('missing_values_pct', 0)
            
            if missing_pct < 5:
                recommendations.append("âœ… ë°ì´í„° í’ˆì§ˆì´ ìš°ìˆ˜í•©ë‹ˆë‹¤.")
            elif missing_pct < 10:
                recommendations.append("âš ï¸  ì¼ë¶€ ë°ì´í„° ê²°ì¸¡ê°’ì´ ìˆìŠµë‹ˆë‹¤. ì •ê¸°ì ì¸ ë°ì´í„° ì ê²€ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            else:
                recommendations.append("ğŸ”§ ë°ì´í„° ê²°ì¸¡ê°’ ë¹„ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤. ë°ì´í„° ì •ì œë¥¼ ê°•í™”í•˜ì„¸ìš”.")
        
        # ì¼ë°˜ì ì¸ ìš´ì˜ ê¶Œì¥ì‚¬í•­
        recommendations.extend([
            "ğŸ”„ ì •ê¸°ì ì¸ ëª¨ë¸ ì¬í›ˆë ¨ ìŠ¤ì¼€ì¤„ì„ ìˆ˜ë¦½í•˜ì„¸ìš”.",
            "ğŸ“Š ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ ì¶”ì í•˜ì„¸ìš”.",
            "ğŸ’¾ ì¤‘ìš” ë°ì´í„°ì™€ ëª¨ë¸ì˜ ë°±ì—… ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”.",
            "ğŸ“ˆ ì‚¬ìš©ëŸ‰ ì¦ê°€ì— ëŒ€ë¹„í•œ í™•ì¥ ê³„íšì„ ì¤€ë¹„í•˜ì„¸ìš”."
        ])
        
        self.benchmark_results['final_recommendations'] = recommendations
        return recommendations
    
    def generate_comprehensive_report(self):
        """ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ ìƒì„±"""
        logger.info("ğŸ“‹ ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        end_time = datetime.now()
        total_benchmark_time = (end_time - self.start_time).total_seconds()
        
        # ì‹œìŠ¤í…œ ì •ë³´
        system_info = self.benchmark_results.get('system_info', {})
        memory_gb = system_info.get('memory', {}).get('total', 0) / (1024**3)
        
        # ë°°í¬ ì¤€ë¹„ë„
        deployment = self.benchmark_results.get('deployment_readiness', {})
        readiness_score = deployment.get('overall_readiness_score', 0)
        readiness_status = deployment.get('deployment_recommendation', 'Unknown')
        
        # ëª¨ë¸ ì¸ë²¤í† ë¦¬
        models = self.benchmark_results.get('model_inventory', {})
        total_models = models.get('total_model_count', 0)
        total_size_mb = models.get('total_model_size_mb', 0)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        performance = self.benchmark_results.get('performance_benchmarks', {})
        current_memory = performance.get('memory_efficiency', {}).get('current_memory_mb', 0)
        
        report = f"""
ğŸ¢ ì„œìš¸ ì‹œì¥ ìœ„í—˜ë„ ML ì‹œìŠ¤í…œ - ìµœì¢… ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ
{'='*70}

ğŸ“Š ì‹œìŠ¤í…œ ê°œìš”:
  â€¢ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ ì‹œê°„: {end_time.strftime('%Y-%m-%d %H:%M:%S')}
  â€¢ ì´ ì†Œìš” ì‹œê°„: {total_benchmark_time:.1f}ì´ˆ
  â€¢ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {memory_gb:.1f}GB
  â€¢ í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {current_memory:.1f}MB

ğŸ¤– ëª¨ë¸ í˜„í™©:
  â€¢ ì´ ëª¨ë¸ ê°œìˆ˜: {total_models}ê°œ
  â€¢ ì´ ëª¨ë¸ í¬ê¸°: {total_size_mb:.1f}MB
  â€¢ ê¸€ë¡œë²Œ ëª¨ë¸: {len(models.get('global_models', []))}ê°œ
  â€¢ ì§€ì—­ ëª¨ë¸: {len(models.get('regional_models', []))}ê°œ  
  â€¢ ë¡œì»¬ ëª¨ë¸: {len(models.get('local_models', []))}ê°œ

ğŸš€ ë°°í¬ ì¤€ë¹„ë„:
  â€¢ ë°°í¬ ì¤€ë¹„ ì ìˆ˜: {readiness_score}/100ì 
  â€¢ ë°°í¬ ìƒíƒœ: {readiness_status}
  
ğŸ“ˆ ë°ì´í„° ìƒíƒœ:
"""
        
        # ë°ì´í„° ê²€ì¦ ê²°ê³¼
        data_validation = self.benchmark_results.get('data_validation', {})
        if data_validation.get('processed_data_exists', False):
            quality_metrics = data_validation.get('data_quality_metrics', {})
            total_rows = quality_metrics.get('total_rows', 0)
            missing_pct = quality_metrics.get('missing_values_pct', 0)
            
            report += f"""  â€¢ ì²˜ë¦¬ëœ ë°ì´í„°: âœ… {total_rows:,} í–‰
  â€¢ ë°ì´í„° í’ˆì§ˆ: ê²°ì¸¡ê°’ {missing_pct:.2f}%
  â€¢ í•„ìˆ˜ ì»¬ëŸ¼: {'âœ… ëª¨ë‘ ì¡´ì¬' if data_validation.get('data_consistency', {}).get('required_columns_present', False) else 'âŒ ì¼ë¶€ ëˆ„ë½'}
"""
        else:
            report += "  â€¢ ì²˜ë¦¬ëœ ë°ì´í„°: âŒ ì—†ìŒ\n"
        
        # ìµœì¢… ê¶Œì¥ì‚¬í•­
        recommendations = self.benchmark_results.get('final_recommendations', [])
        report += "\nğŸ’¡ ìµœì¢… ê¶Œì¥ì‚¬í•­:\n"
        for i, rec in enumerate(recommendations, 1):
            report += f"  {i}. {rec}\n"
        
        # ë‹¤ìŒ ë‹¨ê³„
        report += f"""
ğŸ¯ ë‹¤ìŒ ë‹¨ê³„:
  1. ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ë°ì´í„°ë¡œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
  2. API ì—”ë“œí¬ì¸íŠ¸ êµ¬ì¶• ë° í†µí•© í…ŒìŠ¤íŠ¸  
  3. ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ ê°œë°œ
  4. ìš´ì˜ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶•
  5. í”„ë¡œë•ì…˜ í™˜ê²½ ë°°í¬

â° ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ: {end_time.strftime('%Y-%m-%d %H:%M:%S')}
"""
        
        # ë³´ê³ ì„œ ì €ì¥
        report_path = Path("final_system_benchmark_report.txt")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        # ìƒì„¸ ê²°ê³¼ JSON ì €ì¥
        detailed_results = self.benchmark_results.copy()
        detailed_results['timestamp'] = self.benchmark_results['timestamp'].isoformat()
        detailed_results['completion_time'] = end_time.isoformat()
        detailed_results['total_benchmark_time'] = total_benchmark_time
        
        json_path = Path("detailed_system_benchmark.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, indent=2, ensure_ascii=False, default=str)
        
        return report
    
    def run_full_benchmark(self):
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        logger.info("ğŸš€ ì„œìš¸ ì‹œì¥ ìœ„í—˜ë„ ML ì‹œìŠ¤í…œ ìµœì¢… ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        logger.info("="*70)
        
        try:
            # 1. ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
            self.collect_system_info()
            
            # 2. ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
            self.validate_data_integrity()
            
            # 3. ëª¨ë¸ ì¸ë²¤í† ë¦¬ í™•ì¸
            self.inventory_models()
            
            # 4. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
            self.run_performance_benchmarks()
            
            # 5. ë°°í¬ ì¤€ë¹„ ìƒíƒœ í‰ê°€
            self.assess_deployment_readiness()
            
            # 6. ìµœì¢… ê¶Œì¥ì‚¬í•­ ìƒì„±
            recommendations = self.generate_final_recommendations()
            
            # 7. ì¢…í•© ë³´ê³ ì„œ ìƒì„±
            report = self.generate_comprehensive_report()
            logger.info("\n" + report)
            
            return {
                'benchmark_results': self.benchmark_results,
                'recommendations': recommendations
            }
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ë²¤ì¹˜ë§ˆí¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    benchmark = SystemBenchmark()
    results = benchmark.run_full_benchmark()
    
    deployment_score = results['benchmark_results'].get('deployment_readiness', {}).get('overall_readiness_score', 0)
    
    print(f"\nğŸ¯ ìµœì¢… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
    print(f"  ë°°í¬ ì¤€ë¹„ ì ìˆ˜: {deployment_score}/100ì ")
    print(f"  ê¶Œì¥ì‚¬í•­: {len(results['recommendations'])}ê°œ")
    print(f"  ìƒì„¸ ë³´ê³ ì„œ: final_system_benchmark_report.txt")
    
    return 0


if __name__ == "__main__":
    exit(main())